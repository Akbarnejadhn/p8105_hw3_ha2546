---
title: "p8105_hw3_ha2546"
author: "Hana Akbarnejad"
date: "10/9/2019"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(viridis)
library(ggridges)
library(p8105.datasets)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
  )

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_classic())
```

## Problem1
```{r Prob1_desc}

data("instacart")

instacart_desc = instacart %>% 
  filter(
    order_id == 112
  )
```

**Dataset Description**


In the first problem, we are interested in exploring the _instacart_ dataset. This dataset stores information about online grocery shopping in 2017. The datset contains `r nrow(instacart)` observations of `r n_distinct(pull(instacart, order_id))` unique orders of `r n_distinct(pull(instacart, product_name))` differennt products. There are `r ncol(instacart)` variables associated with these rows such as _order_dow_ (the day of the week that the order has been placed), _order_hour_of_day_ (the time of the day that the order has been placed), _product_name_ (the item purchased), _aisle_ (the aisle that the item has chosen from), etc. For example, we can see that ordere with _order_id_ 112 contained `r nrow(instacart_desc)` items including `r head(pull(instacart_desc, product_name), 3)` from `r head(pull(instacart_desc, department), 3)` departments. These products has been chosen from aisles `r head(pull(instacart_desc, aisle_id), 3)` which are associated with the `r head(pull(instacart_desc, aisle), 3)` aisles, respectively. This order has been placed on Thursday at `r head(pull(instacart_desc, order_hour_of_day), 1)`, and `r head(pull(instacart_desc, days_since_prior_order), 1)` days after the pripr order of this customer (with the customer ID: `r head(pull(instacart_desc, user_id), 1)`).

**Parts a and b**

```{r problem1_parta&b}

aisle_fav_n = instacart %>% 
  # group_by(aisle) %>% 
  count(aisle, name = "aisle_order_n") %>% 
  arrange(desc(aisle_order_n))

aisle_fav_plot = aisle_fav_n %>% 
  filter(aisle_order_n > 10000) %>% 
  ggplot(aes(x = reorder(aisle, -aisle_order_n), y = aisle_order_n)) +
  geom_bar(stat="identity", width=0.5, fill="blue") +
  theme(axis.text.x = element_text(angle = 80, hjust = 1)) +
  labs(
    title = "Number of Total Orders from Favorite Aisles",
    x = "Aisle Name",
    y = "Number of Orders",
    caption = "only aisles with more than 10000 items ordered included"
  )

aisle_fav_n
aisle_fav_plot
```

In this part, I wanted to figure out how many distinct aisles we have in the dataset, and which were the favorite ones. So, I grouped the dtaset by aisles, and understood that there are total of `r nrow(aisle_fav_n)` different isles. Then I counted the number of times that have been ordered from each isle. By further arranging this column in descending order, I found the most popular isles. Top five aisles with the most items ordered from are `r head(pull(aisle_fav_n, aisle),10)`.I then made a plot that shows the number of items ordered in each aisle (only for aisles with more than 10000 items ordered). For this purpose I made the above bar plot that shows the aisle names on x axis and the number of items ordered on y axis (note that only aisles with more than 10000 items ordered included).


**Part c**

```{r problem1_partc}

top3_items = instacart %>% 
  filter(aisle == c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>%
  summarize(
    count = n()
    ) %>% 
  top_n(3, count) %>% 
  knitr::kable()

top3_items
```

In part c of problem 1 was focused on figuring out the top 3 items which was ordered from aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. I grouped the data by aisle and product names, counted the number of times each item was ordered and presented the result in the table above.

**Part d**

```{r problem1_b, warning = FALSE}

hours_days_products = instacart %>% 
  filter(product_name == c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(
    hours_mean = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = hours_mean
  ) %>%
    rename(
      Sun = `0`, Mon = `1`, Tue = `2`, Wed = `3`, Thr = `4`, Fri = `5`, Sat = `6`
      ) %>% 
  knitr::kable(digits = 1)

hours_days_products
```

In the last part, I worked on the finding the mean hour of day at which _Pink Lady Apples_ and _Coffee Ice Cream_ were ordered on each day of the week. I made a table which is easy to grasp, with days of the week as variables and mentioned products as rows, including the average hour of day at which the items were ordered.

## Problem 2

In this part, I worked with _BRFSS_ dataset. First, I cleaned the names and made a subset of the data which is focused on the _Overall Health_, then I transformed response levels as ordered factors from poor to excellent.

```{r problem2_a}

data("brfss_smart2010") 

brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"), ordered = TRUE)
  )
```

On the next step, I needed to know which states has more or equal than 7 locations in both 2002 and 20010.

```{r}
brfss_location_2002 = brfss_data %>%
  filter(year == "2002") %>% 
  group_by(locationabbr) %>% 
  summarize(
    distinct_location = n_distinct(locationdesc)
    ) %>% 
  filter(distinct_location >= 7)


brfss_location_2010 = brfss_data %>%
  filter(year == "2010") %>% 
  group_by(locationabbr) %>% 
  summarize(
    distinct_location = n_distinct(locationdesc)
    ) %>% 
  filter(distinct_location >= 7)
```

We observed that during 2002, there were `r nrow(brfss_location_2002)` locations satissfying this condition, and during 2010 this number was `r nrow(brfss_location_2010)`.

In this part, I made a subset of data from excellent responses, then I summarized the mean of the data_value variable in different US states and then made a spagetti plot that shows the mean of crude prevalence percentage ( _data_value_ ) against different years.
```{r}

  brfss_exc = brfss_data %>%
  filter(
    response == "Excellent"
  ) %>%
  group_by(locationabbr, year) %>%
  summarize(
    mean_exc_location = mean(data_value)
  )

brfss_exc_spagetti = brfss_exc %>% 
  ggplot(aes(x = year, y = mean_exc_location, color = locationabbr)) +
  geom_line() +
    labs(
    title = "Average Crude Prevalence 2002 - 2010 among US States",
    x = "Year",
    y = "Average Crude Prevalence (%)",
    caption = "From BRFSS Excellent Responses")

brfss_exc_spagetti + labs(color = "STATES")

```

In this part, I made a panel plot for the data from 2006 and 2010, focusing on the distribution of crude prevalence percentage for different response levels ("Poor", "Fair", "Good", "Very good", "Excellent"), among different counties of Ny State. 
```{r panel_plot}

library("patchwork")

plot_2006 = brfss_data %>%
  filter(
    year == "2006",
    locationabbr == "NY"
  ) %>%
  group_by(locationdesc, response) %>%
  summarize(
    mean_value = mean(data_value)
  ) %>% 
  ggplot(aes(x = response, y = mean_value, color = locationdesc, group = locationdesc)) +
  geom_point(alpha = .5) +
  geom_line() +
  labs(
    title = "Distribution of Crude Prevalence in NY Counties - 2006",
    x = "Response Level",
    y = "Crude Prevalence (%)",
    caption = NULL
  ) +
  guides(color = FALSE, size = FALSE)


plot_2010 = brfss_data %>%
  filter(
    year == "2010",
    locationabbr == "NY"
  ) %>%
  group_by(locationdesc, response) %>%
  summarize(
    mean_value = mean(data_value)
  ) %>% 
  ggplot(aes(x = response, y = mean_value, color = locationdesc, group = locationdesc)) +
  geom_point(alpha = .5) +
  geom_line() +
  labs(
    title = "Distribution of Crude Prevalence in NY Counties - 2010",
    x = "Response Level",
    y = "Crude Prevalence (%)",
    caption = NULL
  ) +
  theme(legend.position = "bottom")


(plot_2006 / plot_2010)
```

## Problem 3
```{r prob3_load}
accel_data = read_csv("./data/accel_data.csv") 

accel_data_tidy = accel_data %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = activity_1:activity_1440,
    names_to = "activity_number",
    values_to = "value",
    names_prefix = "activity_"
  ) %>%
  select(activity_number, week, day, day_id, value) %>% 
  mutate(
    is_weekend = (day == "Saturday" | day == "Sunday")
  )

accel_data_tidy
```
 DESCRIBE IT
 
```{r}

total_activity_table = accel_data_tidy %>% 
  group_by(day_id) %>% 
  summarise(
    total_activity = sum(value)
  )

total_activity_plot = total_activity_table %>% 
  ggplot(aes(x = day_id, y = total_activity)) +
  geom_point() + 
  geom_line()

total_activity_table
total_activity_plot
```
Describe Trends

```{r}

activity_day_amount = accel_data_tidy %>% 
  mutate(
    activity_number = as.numeric(as.character(activity_number))
  ) %>% 
  group_by(day, activity_number) %>% 
  summarize(
    mean_activity_day = mean(value)
  ) %>% 
  ggplot(aes(x = activity_number, y = mean_activity_day, color = day, group = day)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Average Activity over 24 hours for Each Day",
    x = "Time Course (hours)",
    y = "Average Activity Amount",
    caption = "Measured over 5 weeks"
  ) +
  scale_x_discrete(limit = c(240, 480, 720, 960, 1200, 1440),
                   labels = c("4", "8", "12", "16", "20", "24"))

activity_day_amount
```

